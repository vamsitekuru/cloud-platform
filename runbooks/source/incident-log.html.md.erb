---
title: Incident Log
weight: 45
last_reviewed_on: 2020-05-13
review_in: 3 months
---

# Incident Log


## April 2020

### Incident on 2020-04-215 10:58 UTC

- **Status**: Resolved at 2020-04-215 15:09 UTC

- **Incident**: After an upgrade of the Nginx ingresses, support for legacy TLS was dropped.

- **Impact**:
    - IE11 users could not access any services running on the Cloud Platform 
    - A few teams came forward with the issue :
    --- LAA
    --- Correspondence Tool
    --- Prisoner Money

- **Context**:
    * After an upgrade of the Nginx Helm chart v1.24.0 to v1.35
    * The current version of Nginx has deprecated support for TLS < 1.3
    * The issue was spotted on IE11 browsers. 
    * Timeline : [https://docs.google.com/document/d/1SCf1WT82IlBYWozWN_FXZqL5h0KUcul_QAkxE84YDw0/edit?usp=sharing](https://docs.google.com/document/d/1SCf1WT82IlBYWozWN_FXZqL5h0KUcul_QAkxE84YDw0/edit?usp=sharing)
    * Slack thread: [https://mojdt.slack.com/archives/C57UPMZLY/p1586954463298700](https://mojdt.slack.com/archives/C57UPMZLY/p1586954463298700)

- **Resolution**:
    The Nginx configuration was modified to enable TLSv1, TLSv1.1 and TLSv1.2


## February 2020

### Incident on 2020-02-25 10:58 UTC

- **Status**: Resolved at 2020-02-25 17:07 UTC

- **Incident**: During an upgrade, new masters were not coming up correctly (missing calico networking and other pods)

- **Impact**:
    - Degraded kubernetes API performance (because some API calls were being directed to non-functioning masters)
    - Increased risk of cluster failure, because we were running on a single master during the incident

- **Context**:
    * Upgrading from kubernetes 1.13.12 to 1.14.10, kops 1.13.2 to 1.14.1
    * The first master was replaced fine, but the second didn't have calico and some other essential pods, and was not functioning correctly
    * Attempting to roll back the upgrade, every new master exhibited the same problem
    * Slack thread: [https://mojdt.slack.com/archives/C514ETYJX/p1582628309085600](https://mojdt.slack.com/archives/C514ETYJX/p1582628309085600)

- **Resolution**:
    The `kube-system` namespace has a label, `openpolicyagent.org/webhook: ignore` This label tells the Open Policy Agent (OPA) that pods are allowed to run in this namespace on the master nodes. Somehow, this label got removed, so the OPA was preventing pods from running on the new master nodes, as each one came up, so the new master was unable to launch essential pods such as `calico` and `fluentd`.

### Incident on 2020-02-18 14:13 UTC

- **Status**: Resolved at 2020-02-18 14:59 UTC

- **Incident**: Pingdom reported that Prometheus was down (prometheus.cloud-platform.service.justice.gov.uk).

- **Impact**:
    - The prometheus dashboard was unavailable for everyone, for the whole duration of the incident.
    - Between 2020-02-18 14:22 and 2020-02-18 14:26, prometheus could not receive metrics.

- **Context**:
   - Although the Prometheus URL was unreachable, Grafana and Alertmanager were resolving.
   - There seemed to be an issue preventing requests to reach the prometheus pods.
   - Disk space and other resources, the usual suspects, were ruled out as the cause.
   - The domain name amd ingress were both valid.

- **Resolution**:
    We suspect an intermittent & external networking issue to be the cause of this outage.


### Incident on 2020-02-12 11:45 UTC

- **Status**: Resolved at 2020-02-18 12:07 UTC

- **Identified**: Pingdom reported Concourse (concourse.cloud-platform.service.justice.gov.uk) down.

- **Context**: One of the engineers was deleting old clusters (he ran `terraform destroy`) and he wasn't fully aware in which _terraform workspace_ was working on. Using `terraform destroy`, EKS nodes/workers were deleted from the manager cluster.

- **Resolution**: Using terraform (`terraform apply -var-file vars/manager.tfvars` specifically) the cluster nodes where created and the infrastructure aligned? to the desired terraform state


